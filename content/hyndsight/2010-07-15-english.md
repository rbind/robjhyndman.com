---
author: robjhyndman
comments: true
date: 2010-07-15 00:53:30+00:00

link: https://robjhyndman.com/hyndsight/english/
slug: english
title: The falling standard of English in research
wordpress_id: 866
categories:
- journals
- writing
---

It seems that most journals no longer do any serious copy-editing, and the standard of English is falling. Today I was reading an article from the _[European Journal of Operational Research](http://www.journals.elsevier.com/european-journal-of-operational-research/)_, which is supposedly a good OR journal (current impact factor over 2). Take this for an example from the first page of [this paper](http://dx.doi.org/10.1016/j.ejor.2010.03.026):



>If the learned patterns are unstable, the learning tools would produce inconsistent concepts. To overcome this difficult situation, we employed artificial neural networks (ANNs, NNs) for helping the learning task. NNs have attracted a lot of attention form academic researchers and industrial practitioners because of the powerful flexible nonlinear modeling capability ([Balestrassi et al., 2009], [Bellini and Figa-Talamanca, 2005] and [Qi and Zhang, 2001]). It is the main reason for their popularity that the data driven tools have less restriction when applying. Learning tools with the stable training base usually have reliable performances.



The paper continues in this vein for ten pages, culminating in an equally remarkable conclusion:

>With the sample size growing, the shadow set contains a large number of functional, virtual data, instead of whole real data. It would possess less population representation then. Before establishing the theoretical basis, we used the trial-and-error way for the expedient explanation and concluded that the virtual data size should be 10 at most in this case.


How did that get past the associate editor, editor, copy-editor and typesetter? Did everyone really think it was ok, or did the paper get published without any of them actually reading it properly? It sounds like something out of an automatic translation program such as [Google translate](http://translate.google.com/), although I suspect that Google translate may do rather better.

The desperate rush to publish as much and as often as possible has led to a deluge of badly expressed sentences, cobbled together to look like an article, but often expressing little of value.

Of course, one of the reasons for the rise of barely readable English is the increasing number of papers written by researchers whose first language is not English. I feel for them---I couldn't write a single sentence in any other language. However, there are services available to help. In fact, in the submission guidelines for Elsevier journals, authors are advised to visit [http://webshop.elsevier.com/languageediting/](http://webshop.elsevier.com/languageediting/) if they need assistance.

For authors who can't afford to use such services, and even for authors whose English is passable, journals need copy-editors. Unfortunately, it seems the problem is often the poor quality of the work done by copy-editors employed by the journal publishers.

One of the first things I did when I took over as Editor-in-Chief at the _International Journal of Forecasting_ was replace the copy-editing team employed by Elsevier (the same group responsible for the above paragraphs) and install my own copy-editor who can at least recognize bad English when she sees it. Furthermore, I convinced Elsevier that they should pay for her. As a result, I think our published papers are now of a much higher quality than they were a few years ago. Hopefully that means they are read more, cited more and have greater impact. I wish other journals would do the same.
